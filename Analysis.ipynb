{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a13195f",
   "metadata": {},
   "source": [
    "# Building, Training, and Deploying a Machine Learning Model with Amazon SageMaker\n",
    "\n",
    "<br>\n",
    "\n",
    "**Table of Contents:**\n",
    "\n",
    "- [Libraries](#Libraries)\n",
    "- [Defining Environment Variables](#Defining-Environment-Variables)\n",
    "- [Creating an S3 Bucket](#Creating-an-S3-Bucket)\n",
    "- [Retrieving the Dataset](#Retrieving-the-Dataset)\n",
    "- [Splitting into Training/Test Sets](#Splitting-into-Training/Test-Sets)\n",
    "- [Training the Model](#Training-the-Model)\n",
    "- [Deploying the Model](#Deploying-the-Model)\n",
    "- [Evaluating Model Performance](#Evaluating-Model-Performance)\n",
    "- [Cleaning up](#Cleaning-up)\n",
    "    - [Deleting Endpoint](#Deleting-Endpoint)\n",
    "    - [Deleting Training Artifacts and S3 Bucket](#Deleting-Training-Artifacts-and-S3-Bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd7941",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Libraries\n",
    "\n",
    "We start the notebook by importing the necessary libraries and tools. The AWS-specific tools are:\n",
    "\n",
    "- [`Boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html): the official AWS Software Development Kit (SDK) for Python. Boto3 enables developers to integrate Python applications, libraries, or scripts with AWS services, such as Amazon S3 and Amazon EC2.\n",
    "\n",
    "- [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/): an open-source library for training and deploying machine learning models on Amazon SageMaker.\n",
    "\n",
    "<br>\n",
    "\n",
    "From Sagemaker, we will separately import the `get_execution_role()` function and the `CSVSerializer`. The former, as the name suggests, allows us to extract the execution role for the notebook instance, i.e. the IAM role that we created for our notebook instance. This role will later get passed to the tuning job. The latter, serialises data of various formats to a CSV-formatted string for the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dcb627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "import re, os, urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbf222",
   "metadata": {},
   "source": [
    "Moreover, we will install and import [`watermark`](https://github.com/rasbt/watermark), an IPython magic extension that enables us to print version numbers and hardware information. The cell magic command `%%capture` is placed at the beginning of the following cell to suppress its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9145139",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b10f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re         2.2.1\n",
      "matplotlib 3.3.4\n",
      "numpy      1.19.5\n",
      "boto3      1.21.42\n",
      "pandas     1.1.5\n",
      "sagemaker  2.86.2\n",
      "watermark  2.0.2\n",
      "Fri Jun 10 2022 \n",
      "\n",
      "CPython 3.6.13\n",
      "IPython 7.16.1\n",
      "\n",
      "compiler   : GCC 9.3.0\n",
      "system     : Linux\n",
      "release    : 4.14.252-131.483.amzn1.x86_64\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 2\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "import watermark\n",
    "%load_ext watermark\n",
    "\n",
    "# See version of system, Python, and libraries\n",
    "%watermark -n -v -m -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad6f9f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Defining Environment Variables\n",
    "\n",
    "We continue by defining the environment variables we need to prepare the data, train the ML model, and deploy the ML model.\n",
    "\n",
    "Firstly, we define the execution role for the notebook instance using the `get_execution_role()` function. This is the IAM role we created for this particular notebook instance; it will later be passed to the tunning job.\n",
    "\n",
    "Then, we extract the name of the AWS region where our instance is hosted. This variable is necessary for building an XGBoost container (see next paragraph) and later for creating an S3 bucket.\n",
    "\n",
    "Finally, we build an XGBoost container using `sagemaker`'s `image_uris.retrieve` function. SageMakers uses Docker containers for training and deploying machine learning algorithms. Containers allow developers and data scientists to package software into standardised units that run consistently on any platform that supports Docker.\n",
    "\n",
    "In this project, we will use the XGBoost built-in algorithm to build an XGBoost training container. We can automatically spot the XGBoost built-in algorithm image URI using the SageMaker `image_uris.retrieve` API. After specifying the XGBoost image URI, you can use the XGBoost container to construct an estimator using the SageMaker `Estimator` API and initiate a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8e4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-east-1 region.\n",
      "You will use the 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "\n",
    "my_region = boto3.session.Session().region_name\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "xgboost_container = sagemaker.image_uris.retrieve('xgboost', my_region, 'latest')\n",
    "\n",
    "print(f'Success - the MySageMakerInstance is in the {my_region} region.')\n",
    "print(f'You will use the {xgboost_container} container for your SageMaker endpoint.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8767c960",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Creating an S3 Bucket\n",
    "\n",
    "In this section, we create an Amazon S3 bucket to store our data.\n",
    "Simply put, an S3 bucket is a storage location to hold files. S3 files are referred to as objects.\n",
    "\n",
    "Using the `boto3` library with Amazon S3 allows us to create, update, and delete S3 Buckets and objects from Python programs with ease. To establish a connection with S3, we need to use *resources*, i.e. an object-oriented interface to Amazon Web Services. For this purpose, we invoke the `resource()` method of a Session and pass in a service name, as in the following example:\n",
    "\n",
    "```s3 = boto3.resource('s3')```\n",
    "\n",
    "Once a connection with S3 is established, we can create a bucket. \n",
    "\n",
    "Note that every Amazon S3 Bucket must have a unique name across all AWS accounts and customers. Therefore, we need to use a name that hasn’t been taken yet, otherwise the program will yield an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1f5d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: S3 bucket created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'sagemaker-tutorial-12345'  # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "try:\n",
    "    if my_region == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': my_region})\n",
    "    print('Success: S3 bucket created successfully!')\n",
    "except Exception as e:\n",
    "    print('S3 error: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeef990",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Retrieving the Dataset\n",
    "\n",
    "The next task is downloadinf the data to our SageMaker notebook instance and loading the data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e104180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Downloaded bank_clean.csv.\n",
      "Success: Data loaded into dataframe.\n"
     ]
    }
   ],
   "source": [
    "dataset_url = 'https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv'\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(url=dataset_url, filename='bank_clean.csv')\n",
    "    print('Success: Downloaded bank_clean.csv.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ', e)\n",
    "\n",
    "try:\n",
    "    model_data = pd.read_csv('./bank_clean.csv', index_col=0)\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54211952",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Splitting into Training/Test Sets\n",
    "\n",
    "\n",
    "\n",
    "The dataset is already processed, so we can skip typical machine learning pre-processing practises (such as encoding categorical features). The only modification we need to perform is randomly splitting the dataset into a training and a test set. The training data (70% of all customers) is used during the model training. The test data (remaining 30% of customers) is used to evaluate the performance of the trained model and measure how well it generalises to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8a36cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 28831 rows x 61 columns.\n",
      " Test Set: 12357 rows x 61 columns.\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729),\n",
    "                                 [int(0.7 * len(model_data))])\n",
    "\n",
    "print(f'Train Set: {train_data.shape[0]} rows x {train_data.shape[1]} columns.')\n",
    "print(f' Test Set: {test_data.shape[0]} rows x {test_data.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c240ad",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78518d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5541659b",
   "metadata": {},
   "source": [
    "We initialize a session, which allows us to manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
    "\n",
    "Then, we initialize an Estimator instance. We need to specify the following parameters:\n",
    "\n",
    "- `image_uri`: The container image to use for training.\n",
    "- `role`: The AWS IAM role we defined earlier.\n",
    "- `instance_count`: Number of Amazon EC2 instances to use for training.\n",
    "- `instance_type`: Type of EC2 instance to use for training.\n",
    "- `output_path`: S3 location for saving the training result (model artifacts and output files).\n",
    "- `sagemaker_session`: Session object which manages interactions with Amazon SageMaker APIs and any other AWS services needed.\n",
    "\n",
    "After creating the Estimator, we also define the model’s hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac77dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "xgb = sagemaker.estimator.Estimator(image_uri=xgboost_container,\n",
    "                                    role=role,\n",
    "                                    instance_count=1,\n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket_name, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        verbosity=2,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19582029",
   "metadata": {},
   "source": [
    "We are now ready to start the training job. The following command trains the model using gradient optimization on a `ml.m4.xlarge` instance. After a few minutes, we should see the training logs being generated in our Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7438234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-10 12:18:25 Starting - Starting the training job...ProfilerReport-1654863505: InProgress\n",
      "...\n",
      "2022-06-10 12:19:09 Starting - Preparing the instances for training.........\n",
      "2022-06-10 12:20:50 Downloading - Downloading input data...\n",
      "2022-06-10 12:21:21 Training - Downloading the training image......\n",
      "2022-06-10 12:22:16 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2022-06-10:12:22:19:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2022-06-10:12:22:19:INFO] Path /opt/ml/input/data/validation does not exist!\u001b[0m\n",
      "\u001b[34m[2022-06-10:12:22:19:INFO] File size need to be processed in the node: 3.38mb. Available memory size in the node: 8461.46mb\u001b[0m\n",
      "\u001b[34m[2022-06-10:12:22:19:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[12:22:19] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[12:22:20] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.100482\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.099858\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.099754\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.099095\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.098991\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.099303\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.099684\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.09906\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.098852\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.098679\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.098748\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.098748\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.098748\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.09854\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.098574\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.098609\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.098817\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.098817\u001b[0m\n",
      "\u001b[34m[12:22:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.098679\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.098679\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.098713\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.098505\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.098401\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.098332\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.098332\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.09795\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.098262\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.098193\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 24 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.097985\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.097499\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.097638\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.097395\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.097222\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.097118\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.097014\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.09684\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.096667\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.096736\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.096563\u001b[0m\n",
      "\u001b[34m[12:22:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.096355\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 36 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.096285\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 38 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.096528\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 16 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.096355\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.096459\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.096355\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 34 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.096216\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.096077\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.0958\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.095904\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.095904\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 34 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.095834\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.095765\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.095904\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.095834\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.095834\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.09573\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.095626\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.095696\u001b[0m\n",
      "\u001b[34m[12:22:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.095661\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 24 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.095592\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.095522\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.095383\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 20 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.095314\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.095661\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 32 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.095661\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.095418\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 36 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.095314\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.095349\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 28 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.095314\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 28 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.095314\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 32 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.095383\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.095453\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.095349\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 24 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.095245\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.095175\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 18 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.095071\u001b[0m\n",
      "\u001b[34m[12:22:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.095175\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.095002\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 20 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.095037\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.095037\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.095002\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.094794\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.094759\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.094933\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.09469\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.094759\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.094482\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.094447\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.094482\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.094378\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 28 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.094343\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 28 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.094274\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.094239\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.094169\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.094169\u001b[0m\n",
      "\u001b[34m[12:22:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.094204\u001b[0m\n",
      "\u001b[34m[12:22:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 28 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.094204\u001b[0m\n",
      "\u001b[34m[12:22:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.093927\u001b[0m\n",
      "\u001b[34m[12:22:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 38 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.093927\u001b[0m\n",
      "\u001b[34m[12:22:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.093892\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-06-10 12:22:50 Uploading - Uploading generated training model\n",
      "2022-06-10 12:22:50 Completed - Training job completed\n",
      "Training seconds: 127\n",
      "Billable seconds: 127\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b82392b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Deploying the Model\n",
    "\n",
    "After fiting an XGBoost Estimator, we can host the newly created model in SageMaker. For this purpose, we can call `deploy` on an XGBoost estimator to create a SageMaker endpoint. The endpoint runs a SageMaker-provided XGBoost model server and hosts the model produced by your training script, which was run when we called `fit`.\n",
    "\n",
    "This step may take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ead31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742906e",
   "metadata": {},
   "source": [
    "The next step is making predictions. We first set up a CSV serialiser, which takes numpy arrays (like our testing data) and serialises them to CSV format. \n",
    "\n",
    "Finally, we call the `predict` method that returns the result of inference against our model (stored in the `predictions_array` array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa59d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction array: 12357 rows\n"
     ]
    }
   ],
   "source": [
    "test_data_array = test_data.drop(labels=['y_no', 'y_yes'],\n",
    "                                 axis=1).values  # load the data into an array\n",
    "\n",
    "xgb_predictor.serializer = CSVSerializer()  # set the serializer type\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8')  # make predictions\n",
    "\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',')  # and turn the prediction into an array\n",
    "print(f'Prediction array: {predictions_array.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591a279",
   "metadata": {},
   "source": [
    "The output array stores prediction in the form of probabilities, i.e. values ranging from 0 to 1 describing the probability that a new customer will subscribe to the product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654c637",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Evaluating Model Performance\n",
    "\n",
    "In this section, we evaluate the performance and accuracy of the trained machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb5ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Classification Rate: 89.5%\n",
      "\n",
      "Predicted      No Purchase    Purchase\n",
      "Observed\n",
      "No Purchase    90% (10769)    37% (167)\n",
      "Purchase        10% (1133)     63% (288) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = pd.crosstab(index=test_data['y_yes'],\n",
    "                 columns=np.round(predictions_array),\n",
    "                 rownames=['Observed'],\n",
    "                 colnames=['Predicted'])\n",
    "tn = cm.iloc[0, 0]\n",
    "fn = cm.iloc[1, 0]\n",
    "tp = cm.iloc[1, 1]\n",
    "fp = cm.iloc[0, 1]\n",
    "p = (tp + tn) / (tp + tn + fp + fn) * 100\n",
    "\n",
    "print('\\n{0:<20}{1:<4.1f}%\\n'.format('Overall Classification Rate: ', p))\n",
    "print('{0:<15}{1:<15}{2:>8}'.format('Predicted', 'No Purchase', 'Purchase'))\n",
    "print('Observed')\n",
    "print('{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})'.format(\n",
    "    'No Purchase', tn / (tn + fn) * 100, tn, fp / (tp + fp) * 100, fp))\n",
    "print('{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n'.format(\n",
    "    'Purchase', fn / (tn + fn) * 100, fn, tp / (tp + fp) * 100, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66698c5a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Cleaning up\n",
    "\n",
    "### Deleting Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80c01ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87fa31",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Deleting Training Artifacts and S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da184940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': 'ZTNRTJRJVCTWBG2Y',\n",
       "   'HostId': 'jQYxyIcKv2s1hOf660/cWcLa4gyynUxmztyr/N0P/hf+pNyEZ+OPxtDUxbcJdIeeTt5YBn2XZ8s=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': 'jQYxyIcKv2s1hOf660/cWcLa4gyynUxmztyr/N0P/hf+pNyEZ+OPxtDUxbcJdIeeTt5YBn2XZ8s=',\n",
       "    'x-amz-request-id': 'ZTNRTJRJVCTWBG2Y',\n",
       "    'date': 'Fri, 10 Jun 2022 12:27:42 GMT',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3',\n",
       "    'connection': 'close'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/profiler-output/system/incremental/2022061011/1654860600.algo-1.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/GPUMemoryIncrease.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/output/model.tar.gz'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/OverallSystemUsage.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/GPUMemoryIncrease.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/StepOutlier.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/Dataloader.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/OverallFrameworkMetrics.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/profiler-output/system/incremental/2022061010/1654857660.algo-1.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/OverallSystemUsage.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/profiler-output/framework/training_job_end.ts'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-report.html'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/IOBottleneck.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-report.ipynb'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/BatchSize.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/GPUMemoryIncrease.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/CPUBottleneck.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/profiler-output/framework/training_job_end.ts'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/LowGPUUtilization.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/OverallFrameworkMetrics.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/StepOutlier.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/output/model.tar.gz'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/OverallFrameworkMetrics.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/MaxInitializationTime.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/output/model.tar.gz'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/CPUBottleneck.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/BatchSize.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/profiler-output/system/incremental/2022061011/1654861680.algo-1.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/StepOutlier.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/MaxInitializationTime.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/Dataloader.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/OverallSystemUsage.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/profiler-output/system/training_job_end.ts'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/profiler-output/system/training_job_end.ts'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/BatchSize.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/MaxInitializationTime.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/IOBottleneck.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/Dataloader.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/LowGPUUtilization.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/Dataloader.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/LoadBalancing.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/train/train.csv'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/MaxInitializationTime.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/IOBottleneck.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/profiler-output/framework/training_job_end.ts'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-report.html'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/BatchSize.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/LowGPUUtilization.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/CPUBottleneck.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-report.ipynb'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/profiler-output/system/incremental/2022061010/1654857720.algo-1.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/LowGPUUtilization.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/CPUBottleneck.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/StepOutlier.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/profiler-output/system/training_job_end.ts'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/LoadBalancing.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/GPUMemoryIncrease.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/profiler-output/system/incremental/2022061012/1654863720.algo-1.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/profiler-output/framework/training_job_end.ts'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-report.ipynb'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/OverallSystemUsage.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/profiler-output/system/training_job_end.ts'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-reports/LoadBalancing.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-reports/OverallFrameworkMetrics.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/profiler-output/system/incremental/2022061012/1654863600.algo-1.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-report.ipynb'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/output/model.tar.gz'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-10-37-19-722/rule-output/ProfilerReport-1654857439/profiler-output/profiler-report.html'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/profiler-output/system/incremental/2022061011/1654860660.algo-1.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/rule-output/ProfilerReport-1654863505/profiler-output/profiler-reports/IOBottleneck.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/rule-output/ProfilerReport-1654861487/profiler-output/profiler-reports/LoadBalancing.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/profiler-output/system/incremental/2022061011/1654860540.algo-1.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-44-47-531/profiler-output/system/incremental/2022061011/1654861620.algo-1.json'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-11-27-18-893/rule-output/ProfilerReport-1654860438/profiler-output/profiler-report.html'},\n",
       "   {'Key': 'sagemaker/DEMO-xgboost-dm/output/xgboost-2022-06-10-12-18-25-032/profiler-output/system/incremental/2022061012/1654863660.algo-1.json'}]}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c666e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
